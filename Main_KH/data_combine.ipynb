{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde1474b-d686-48d8-aa12-bc295b158506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import pyarrow\n",
    "import fastparquet \n",
    "\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from os.path import exists\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54214067-f4b0-475d-9c71-313092aa4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to read config file settings\n",
    "def read_config(Config_File):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(Config_File)\n",
    "    return config\n",
    "\n",
    "configurations = read_config(\"configurations.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e827a34f-a7e8-437e-a70b-33a259bea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check if JSON file is valid\n",
    "def is_json(j_file):\n",
    "  try:\n",
    "    json.load(j_file)\n",
    "  except ValueError as e:\n",
    "    return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1dff6d-21b4-452a-8d5b-97d52d4ea211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to bulk convert transit view jsons to csv\n",
    "def parse_transitview_rt(start_date_time_str, stop_date_time_str):\n",
    "    start_date_time = datetime.strptime(start_date_time_str, '%m/%d/%Y %H:%M')\n",
    "    stop_date_time = datetime.strptime(stop_date_time_str, '%m/%d/%Y %H:%M')\n",
    "\n",
    "    start = time.time()\n",
    "    f_count = 0\n",
    "    errors = \"\"\n",
    "    \n",
    "    # delta time\n",
    "    delta = dt.timedelta(minutes=1)\n",
    "    \n",
    "    to_proc = stop_date_time - start_date_time + delta\n",
    "    to_proc = int(to_proc.total_seconds() / 60)\n",
    "    print('Processing feed files from ' + str(start_date_time) + ' to ' + str(stop_date_time))\n",
    "    print('estimated # of files to process = ' + str(to_proc)) \n",
    "        \n",
    "    # iterate over range of date / time\n",
    "    while (start_date_time <= stop_date_time):\n",
    "\n",
    "        #generate the file name to open\n",
    "        folder = configurations['common_settings']['transitviewall_history_data_root'] + \"\\\\\" \\\n",
    "            + start_date_time.strftime(\"%#m\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%#d\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%H\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%M\") + '\\\\' \n",
    "        \n",
    "        file = folder + 'feed.json'\n",
    "\n",
    "        #load the file name into mem\n",
    "        if exists(file):\n",
    "            f_count +=1\n",
    "            \n",
    "            with open(file) as json_file:\n",
    "                if is_json(json_file):\n",
    "                    json_file.seek(0)\n",
    "                    data = json.load(json_file)\n",
    "                else:\n",
    "                    errors = errors + 'JSON file is not valid or empty: {}\\\\feed.json'.format(folder) + \"\\n\"\n",
    "                    start_date_time += delta\n",
    "                    continue\n",
    "\n",
    "            routes = dict(data['routes'][0])\n",
    "            gtfs_rt_file = pd.DataFrame()\n",
    "\n",
    "            # load json into flat dictionary\n",
    "            if \"routes\" in data:\n",
    "                # go over all routes in a json file\n",
    "                for route in routes.keys():\n",
    "                    gtfs_rt_route = json_normalize(data['routes'], record_path=[route])\n",
    "                    gtfs_rt_route['route'] = route\n",
    "                    gtfs_rt_route['file'] = start_date_time\n",
    "                    gtfs_rt_route['timestamp'] = [datetime.fromtimestamp(int(val), pytz.timezone(\"UTC\")) for val in gtfs_rt_route['timestamp']]\n",
    "                    # create a single DF for file\n",
    "                    gtfs_rt_file = pd.concat([gtfs_rt_file, gtfs_rt_route])\n",
    "\n",
    "            else:\n",
    "                errors = errors + 'entity key is not present in file: {}\\\\feed.json'.format(folder) + \"\\n\"\n",
    "                \n",
    "            gtfs_rt_file.sort_index(axis=1, inplace=True)\n",
    "            csv_f = folder + 'feed.csv'\n",
    "            gtfs_rt_file.to_csv(csv_f, index=False)\n",
    "\n",
    "        else:\n",
    "            # if file is missing print a log\n",
    "            errors = errors + 'JSON file does not exist: {}\\\\feed.json'.format(folder) + \"\\n\"\n",
    "        \n",
    "        end = time.time()\n",
    "        print('processed ' + str(f_count) + ' files (' + str(round((f_count/to_proc)*100, 2)) + '%) in ' +\\\n",
    "              str(int(end-start)) + 'sec (~time remaining: ' + str(int((to_proc-f_count)*(end-start)/f_count)) +\\\n",
    "              'sec) --- now @ file: ' + str(start_date_time), flush=True, end='\\r')\n",
    "        \n",
    "        # increemnt datetime by 1 minute\n",
    "        start_date_time += delta\n",
    "    \n",
    "    if(not errors):\n",
    "        print('\\n\\nNO ERRORS')\n",
    "    else:\n",
    "        print('\\n\\nErrors:')\n",
    "        print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c00e1871-1753-4644-9e05-69fa06dafb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feed files from 2022-10-25 00:00:00 to 2022-10-25 13:30:00\n",
      "estimated # of files to process = 811\n",
      "processed 806 files (99.38%) in 756sec (~time remaining: 4sec) --- now @ file: 2022-10-25 13:30:0000\n",
      "\n",
      "Errors:\n",
      "JSON file does not exist: D:\\DSCI CAPSTONE\\GITHUB (DREXEL)\\DATA SHAREPOINT\\transitviewall-historical-2022\\10\\25\\13\\26\\\\feed.json\n",
      "JSON file does not exist: D:\\DSCI CAPSTONE\\GITHUB (DREXEL)\\DATA SHAREPOINT\\transitviewall-historical-2022\\10\\25\\13\\27\\\\feed.json\n",
      "JSON file does not exist: D:\\DSCI CAPSTONE\\GITHUB (DREXEL)\\DATA SHAREPOINT\\transitviewall-historical-2022\\10\\25\\13\\28\\\\feed.json\n",
      "JSON file does not exist: D:\\DSCI CAPSTONE\\GITHUB (DREXEL)\\DATA SHAREPOINT\\transitviewall-historical-2022\\10\\25\\13\\29\\\\feed.json\n",
      "JSON file does not exist: D:\\DSCI CAPSTONE\\GITHUB (DREXEL)\\DATA SHAREPOINT\\transitviewall-historical-2022\\10\\25\\13\\30\\\\feed.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse_transitview_rt(\"10/25/2022 00:00\", \"10/25/2022 13:30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9023fdb8-63a9-419c-81f6-f1ababd03b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge csv feed files\n",
    "def merge_transitview_rt(start_date_time_str, stop_date_time_str):\n",
    "    start_date_time = datetime.strptime(start_date_time_str, '%m/%d/%Y %H:%M')\n",
    "    stop_date_time = datetime.strptime(stop_date_time_str, '%m/%d/%Y %H:%M')\n",
    "    \n",
    "    start = time.time()\n",
    "    f_count = 0\n",
    "    \n",
    "    # delta time\n",
    "    delta = dt.timedelta(minutes=1)\n",
    "    \n",
    "    csv_list = []\n",
    "    print('reading in CSVs')\n",
    "    # iterate over range of date / time\n",
    "    while (start_date_time <= stop_date_time):\n",
    "\n",
    "        #generate the file name to open\n",
    "        folder = configurations['common_settings']['transitviewall_history_data_root'] + \"\\\\\" \\\n",
    "            + start_date_time.strftime(\"%#m\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%#d\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%H\") + '\\\\' \\\n",
    "            + start_date_time.strftime(\"%M\") + '\\\\' \n",
    "        \n",
    "        file = folder + 'feed.csv'\n",
    "\n",
    "        if exists(file):\n",
    "            csv_list.append(pd.read_csv(file))\n",
    "            \n",
    "        f_count +=1\n",
    "        end = time.time()\n",
    "        print('read ' + str(f_count) + ' files in ' + str(int(end-start)) + ' sec --- now @ file: ' + str(start_date_time), flush=True, end='\\r')\n",
    "            \n",
    "        # increemnt datetime by 1 minute\n",
    "        start_date_time += delta\n",
    "    \n",
    "    print('\\nmerging into single data frame')\n",
    "    csv_merged = pd.concat(csv_list, ignore_index=True)\n",
    "    print(str(round(time.time()-start,0))+' sec elapsed')\n",
    "    \n",
    "    indexDrop = csv_merged[(csv_merged['late'] == 998) | (csv_merged['late'] == 999)].index\n",
    "    csv_merged.drop(indexDrop , inplace=True)\n",
    "    csv_merged['file'] = pd.to_datetime(csv_merged['file'])\n",
    "    csv_merged['timestamp'] = pd.to_datetime(csv_merged['timestamp'])\n",
    "    \n",
    "    print('exporting to single parquet file')\n",
    "    csv_merged.to_parquet('data\\kh_transitview.parquet', compression='gzip')\n",
    "    print(str(round(time.time()-start,0))+' sec elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014a641e-c208-4377-8b31-89cf1f3c82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in CSVs\n",
      "read 7200 files in 93 sec --- now @ file: 2022-09-05 23:59:00\n",
      "merging into single data frame\n",
      "100.0 sec elapsed\n",
      "exporting to single parquet file\n",
      "114.0 sec elapsed\n"
     ]
    }
   ],
   "source": [
    "merge_transitview_rt(\"9/01/2022 00:00\", \"9/05/2022 23:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f87f6b-aee4-496a-8059-03e575c19081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
